# DFS（分布式存储）

## **存储的普遍问题**

**存储处理能力不足**： 传统的IDE的io值是100次/秒，SATA固态磁盘500次/秒，NVMe固态硬盘达到2000-4000次/秒。即时磁盘的io能力再大数十倍，难道能够抗住网站访问高峰期数十万、数百万甚至上亿用户的同时访问么？这还受到网络io能力的限制。

**存储空间能力不足**： 单块磁盘的容量再大，也无法满足用户的正常访问所需的数据容量限制。

## <mark style="color:orange;">分布式文件系统</mark>

随着传输技术发展，操作系统读写数据的方式，不再局限于本地I/O技术，开始支持远距离的TCP/IP方式获取数据。它相当于新增一种可以远距离传输的I/O技术，使得分散的存储设备和用户操作系统可以通过网络方式接入联合在一起，形成更大容量，更易于拓展伸缩的存储系统，对此人们引入分布式文件系统的概念。

<mark style="color:red;">**分布式文件系统**</mark>（Distributed File System，DFS）是指文件系统管理的物理存储资源不一定直接连接在本地节点上，而是通过计算机网络与节点相连；或是若干不同的逻辑磁盘分区或卷标组合在一起而形成的完整的有层次的文件系统。 分布式文件系统的发展现后经历了三个阶段：

* 网络文件系统、
* 共享SAN文件系统、
* 面向对象的并行文件系统。

从本质上来说，分布式文件系统跟传统的文件系统没本质的差别，只不过是需要额外考虑多节点网络连接的可靠性、接入的存储设备的复杂性，需要文件系统、网络环境、存储策略共同协作而已。 由于文件系统与传统一致，网络环境不受控制，所以，我们平常所说的分布式文件系统，也等同于分布式存储系统

## DSS简介

分布式存储系统，是将数据分散存储在多台独立的设备上，从而解决传统的存储系统的容量和性能限制。所以如果存储服务器的可靠性和安全性无法满足大规模存储应用的需要，那么它就会成为分布式文件系统的性能瓶颈。 其实，分布式存储系统可以理解为多台单机存储系统的各司其职、协同合作，统一的对外提供存储的服务。

常见的文件系统

<figure><img src="../../.gitbook/assets/image (5).png" alt=""><figcaption></figcaption></figure>

### 原理解读

<figure><img src="../../.gitbook/assets/image (1) (1).png" alt=""><figcaption></figcaption></figure>



{% tabs %}
{% tab title="存储角色" %}
<mark style="color:blue;">**节点角色：**</mark>当我们将数据存储到分布式系统上的时候，就需要有一个路由机制，能够将我们的请求转交给对应的存储节点上。所以，根据我们对数据在单节点上的存储原理，我们就需要有一个单独的节点来存储所有数据的元数据信息，分布式存储就作为block的存储区域，专门用户数据存储。

&#x20;存储元素据的节点我们把它称为NameNode，

存储具体数据的节点我们称为DataNode。

<mark style="color:blue;">**数据拆分**</mark>： 当我们要存储的数据非常大，比如说5个G，所以我们在存储的时候，将存储的数据信息发送给元数据控制节点，然后元数据控制节点根据自定义的存储策略，将要存储的数据进行拆分(64M一块)也就是数据切片，将切分后的数据作为一个独立的文件，然后基于同样的路由逻辑，将其分散存储到不同的存储节点上。&#x20;

元数据控制节点，在进行数据切分的时候，还需要能够组合起来，所以拆分后的数据块大小、组合时候的偏移信息、路由到的节点等信息都应该有针对性的记录。 在切片的时候，还可以实现并行的存储逻辑效果。每一个数据块都称为一个shard。
{% endtab %}

{% tab title="数据高可用" %}
<mark style="color:purple;">**元数据高可用**</mark>&#x20;

由于NameNode保存了非常重要的数据信息，所以为了避免因为NameNode故障导致的问题，我们一定要对NameNode进行高可用处理。

&#x20;由于元数据非常小(几k)，所以NameNode上的数据是 非常密集而且io量非常小的。所以为了提高数据的查询和存储效率，我们一般将这些数据保存到内存中。所以为了防止主机断电导致数据丢失，所以我们需要随时的进行数据同步到磁盘中，因为没有办法判断每一次到底是哪种数据被访问或更改，所以在磁盘数据同步的时候，随机io是非常大的。&#x20;

同时，为了避免单节点的数据文件丢失，我们需要通过共享存储的方式将数据保存在第三方的存储设备上，同时还需要对数据存储主机进行高可用

<mark style="color:purple;">**数据高可用**</mark>&#x20;

由于我们对元数据进行了数据切片的方式，实现了数据的高效率存取，但是我们知道，一旦这些数据块中，任意丢弃一块，就会导致所有的数据无法正常的使用。所以有必要对这些数据进行高可用的操作。

对于高可用的方式，我们一般会有两种方式来实现数据的冗余。&#x20;

节点级： 通过对主机节点进行高可用，从而实现数据的冗余，这种机制，成本太高了，不值得。&#x20;

数据级： 我们对拆分后的数据块进行副本操作，而且还可以根据节点的数量，自定义冗余的副本数量。这是推荐的。 主角色的数据块称为 primary shard，副本数据块称为 replica shard。

在进行数据块数据冗余的时候，这些副本的策略机制是有元数据节点来进行控制的，当一个DataNode故障的时候：&#x20;

* 如果主shard没有了，从所有的副本shard中选择一个主。&#x20;
* 如果副本shard没有了，再创建一个副本shard即可。&#x20;
* 一旦DataNode节点数量多于副本数量，控制副本数据在另一个DataNode节点上复制一个新的副本 从而保证副本的总体是满足预期的。

为了防止数据副本节点在同一个物理机架上的时候，因为机架故障，导致所有副本无效，所以我们在考虑冗余的时候，还需要考虑地理位置区域的冗余。
{% endtab %}
{% endtabs %}





####

